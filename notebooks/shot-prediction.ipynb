{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ij2gkHjBtVLL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ahCm9CcRttZc",
    "outputId": "b164451e-0a21-40f2-81fe-ad3bd895ff00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781515</td>\n",
       "      <td>0.585182</td>\n",
       "      <td>-0.377416</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.801634</td>\n",
       "      <td>0.583844</td>\n",
       "      <td>-0.346864</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.803092</td>\n",
       "      <td>0.584818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974083</td>\n",
       "      <td>0.038973</td>\n",
       "      <td>0.727149</td>\n",
       "      <td>-0.345964</td>\n",
       "      <td>0.984270</td>\n",
       "      <td>0.039695</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>-0.396382</td>\n",
       "      <td>0.992604</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440957</td>\n",
       "      <td>0.203960</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.438722</td>\n",
       "      <td>0.191672</td>\n",
       "      <td>-0.024764</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.439866</td>\n",
       "      <td>0.191195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.474126</td>\n",
       "      <td>0.928752</td>\n",
       "      <td>-0.225557</td>\n",
       "      <td>0.982506</td>\n",
       "      <td>0.705118</td>\n",
       "      <td>0.875454</td>\n",
       "      <td>0.124557</td>\n",
       "      <td>0.955982</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.424696</td>\n",
       "      <td>0.311120</td>\n",
       "      <td>-0.467895</td>\n",
       "      <td>0.992522</td>\n",
       "      <td>0.432865</td>\n",
       "      <td>0.294632</td>\n",
       "      <td>-0.461224</td>\n",
       "      <td>0.988378</td>\n",
       "      <td>0.437681</td>\n",
       "      <td>0.293425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820323</td>\n",
       "      <td>0.515408</td>\n",
       "      <td>0.935092</td>\n",
       "      <td>-0.096266</td>\n",
       "      <td>0.941868</td>\n",
       "      <td>0.360824</td>\n",
       "      <td>0.932174</td>\n",
       "      <td>-0.283494</td>\n",
       "      <td>0.962177</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958737</td>\n",
       "      <td>0.674987</td>\n",
       "      <td>-0.351112</td>\n",
       "      <td>0.301481</td>\n",
       "      <td>0.954889</td>\n",
       "      <td>0.621089</td>\n",
       "      <td>-0.356924</td>\n",
       "      <td>0.136248</td>\n",
       "      <td>0.963902</td>\n",
       "      <td>0.608717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>1.783472</td>\n",
       "      <td>2.337639</td>\n",
       "      <td>-0.143793</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>1.531482</td>\n",
       "      <td>2.318639</td>\n",
       "      <td>-0.034189</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.453867</td>\n",
       "      <td>0.209480</td>\n",
       "      <td>-0.095277</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.460642</td>\n",
       "      <td>0.196878</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.464144</td>\n",
       "      <td>0.198346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991521</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.965759</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.941402</td>\n",
       "      <td>0.664213</td>\n",
       "      <td>0.864891</td>\n",
       "      <td>-0.348769</td>\n",
       "      <td>0.990185</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.636110</td>\n",
       "      <td>0.265268</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.641752</td>\n",
       "      <td>0.253239</td>\n",
       "      <td>-0.021199</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945748</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>0.697486</td>\n",
       "      <td>-0.023572</td>\n",
       "      <td>0.969140</td>\n",
       "      <td>0.393944</td>\n",
       "      <td>0.942258</td>\n",
       "      <td>0.144790</td>\n",
       "      <td>0.890441</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.296368</td>\n",
       "      <td>-0.555003</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.665013</td>\n",
       "      <td>0.278563</td>\n",
       "      <td>-0.550951</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.666079</td>\n",
       "      <td>0.276262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.697483</td>\n",
       "      <td>0.732398</td>\n",
       "      <td>0.112357</td>\n",
       "      <td>0.975039</td>\n",
       "      <td>0.166634</td>\n",
       "      <td>0.761494</td>\n",
       "      <td>-0.307279</td>\n",
       "      <td>0.998335</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.576041</td>\n",
       "      <td>0.288460</td>\n",
       "      <td>-0.224353</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.583908</td>\n",
       "      <td>0.278263</td>\n",
       "      <td>-0.200934</td>\n",
       "      <td>0.999721</td>\n",
       "      <td>0.586608</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203585</td>\n",
       "      <td>0.462605</td>\n",
       "      <td>0.748331</td>\n",
       "      <td>0.060704</td>\n",
       "      <td>0.272436</td>\n",
       "      <td>0.477370</td>\n",
       "      <td>0.754690</td>\n",
       "      <td>-0.168187</td>\n",
       "      <td>0.341884</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.426483</td>\n",
       "      <td>0.285799</td>\n",
       "      <td>-0.365079</td>\n",
       "      <td>0.994168</td>\n",
       "      <td>0.416943</td>\n",
       "      <td>0.275199</td>\n",
       "      <td>-0.352136</td>\n",
       "      <td>0.993849</td>\n",
       "      <td>0.416726</td>\n",
       "      <td>0.274645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972544</td>\n",
       "      <td>0.602495</td>\n",
       "      <td>0.890713</td>\n",
       "      <td>-0.125095</td>\n",
       "      <td>0.993906</td>\n",
       "      <td>0.229585</td>\n",
       "      <td>0.888474</td>\n",
       "      <td>-0.097694</td>\n",
       "      <td>0.987159</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.425572</td>\n",
       "      <td>0.303122</td>\n",
       "      <td>-0.525600</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.280438</td>\n",
       "      <td>-0.505842</td>\n",
       "      <td>0.999863</td>\n",
       "      <td>0.425376</td>\n",
       "      <td>0.278651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931002</td>\n",
       "      <td>0.596137</td>\n",
       "      <td>0.890161</td>\n",
       "      <td>-0.111321</td>\n",
       "      <td>0.984413</td>\n",
       "      <td>0.226039</td>\n",
       "      <td>0.890382</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>0.953154</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.268641</td>\n",
       "      <td>0.454742</td>\n",
       "      <td>-0.234921</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.253025</td>\n",
       "      <td>0.448042</td>\n",
       "      <td>-0.218890</td>\n",
       "      <td>0.997838</td>\n",
       "      <td>0.252292</td>\n",
       "      <td>0.443633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>0.736338</td>\n",
       "      <td>0.592920</td>\n",
       "      <td>-0.747410</td>\n",
       "      <td>0.156130</td>\n",
       "      <td>0.727507</td>\n",
       "      <td>0.603030</td>\n",
       "      <td>-0.488235</td>\n",
       "      <td>0.148493</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.718325</td>\n",
       "      <td>0.406157</td>\n",
       "      <td>-0.038254</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>0.734936</td>\n",
       "      <td>0.403235</td>\n",
       "      <td>-0.033635</td>\n",
       "      <td>0.997865</td>\n",
       "      <td>0.737823</td>\n",
       "      <td>0.406299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449575</td>\n",
       "      <td>0.290581</td>\n",
       "      <td>0.589076</td>\n",
       "      <td>-0.304369</td>\n",
       "      <td>0.200060</td>\n",
       "      <td>0.228219</td>\n",
       "      <td>0.519589</td>\n",
       "      <td>-0.377512</td>\n",
       "      <td>0.641665</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.561832</td>\n",
       "      <td>0.290768</td>\n",
       "      <td>0.158343</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.559593</td>\n",
       "      <td>0.278882</td>\n",
       "      <td>0.153861</td>\n",
       "      <td>0.999107</td>\n",
       "      <td>0.557519</td>\n",
       "      <td>0.277077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964355</td>\n",
       "      <td>0.379064</td>\n",
       "      <td>0.903139</td>\n",
       "      <td>-0.242399</td>\n",
       "      <td>0.960533</td>\n",
       "      <td>0.590812</td>\n",
       "      <td>0.913470</td>\n",
       "      <td>-0.190760</td>\n",
       "      <td>0.879906</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.347349</td>\n",
       "      <td>0.302304</td>\n",
       "      <td>-0.655920</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.342640</td>\n",
       "      <td>0.260768</td>\n",
       "      <td>-0.660899</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.349008</td>\n",
       "      <td>0.253441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882690</td>\n",
       "      <td>0.929926</td>\n",
       "      <td>0.550429</td>\n",
       "      <td>-0.217673</td>\n",
       "      <td>0.979205</td>\n",
       "      <td>0.514133</td>\n",
       "      <td>1.053639</td>\n",
       "      <td>-0.407969</td>\n",
       "      <td>0.938037</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.285701</td>\n",
       "      <td>-0.556421</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.444904</td>\n",
       "      <td>0.264486</td>\n",
       "      <td>-0.536918</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.447445</td>\n",
       "      <td>0.263364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987457</td>\n",
       "      <td>0.563189</td>\n",
       "      <td>0.915141</td>\n",
       "      <td>-0.121690</td>\n",
       "      <td>0.997384</td>\n",
       "      <td>0.190810</td>\n",
       "      <td>0.825085</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>0.995471</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541733</td>\n",
       "      <td>0.312349</td>\n",
       "      <td>-0.381705</td>\n",
       "      <td>0.999128</td>\n",
       "      <td>0.553890</td>\n",
       "      <td>0.297387</td>\n",
       "      <td>-0.399631</td>\n",
       "      <td>0.998945</td>\n",
       "      <td>0.558469</td>\n",
       "      <td>0.298456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745611</td>\n",
       "      <td>0.804409</td>\n",
       "      <td>0.830813</td>\n",
       "      <td>0.255628</td>\n",
       "      <td>0.284838</td>\n",
       "      <td>0.402060</td>\n",
       "      <td>0.916217</td>\n",
       "      <td>-0.304885</td>\n",
       "      <td>0.750260</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.486044</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>-0.651600</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.490568</td>\n",
       "      <td>0.261431</td>\n",
       "      <td>-0.629694</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.259873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995542</td>\n",
       "      <td>0.606093</td>\n",
       "      <td>0.920141</td>\n",
       "      <td>-0.060707</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.104275</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>-0.388223</td>\n",
       "      <td>0.998727</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.582448</td>\n",
       "      <td>0.287119</td>\n",
       "      <td>-0.036671</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.592861</td>\n",
       "      <td>0.276927</td>\n",
       "      <td>-0.001321</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>0.595638</td>\n",
       "      <td>0.277430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789295</td>\n",
       "      <td>0.453279</td>\n",
       "      <td>0.733115</td>\n",
       "      <td>0.044083</td>\n",
       "      <td>0.561363</td>\n",
       "      <td>0.480221</td>\n",
       "      <td>0.747601</td>\n",
       "      <td>-0.208855</td>\n",
       "      <td>0.760426</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.639713</td>\n",
       "      <td>0.274084</td>\n",
       "      <td>-0.331927</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.663728</td>\n",
       "      <td>0.273007</td>\n",
       "      <td>-0.349980</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.669219</td>\n",
       "      <td>0.278887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707585</td>\n",
       "      <td>0.430985</td>\n",
       "      <td>1.039195</td>\n",
       "      <td>-0.353327</td>\n",
       "      <td>0.703965</td>\n",
       "      <td>-0.035826</td>\n",
       "      <td>0.653020</td>\n",
       "      <td>-0.390210</td>\n",
       "      <td>0.649675</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.506971</td>\n",
       "      <td>0.271956</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.504271</td>\n",
       "      <td>0.256374</td>\n",
       "      <td>0.032579</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.509093</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298918</td>\n",
       "      <td>0.557836</td>\n",
       "      <td>0.870850</td>\n",
       "      <td>0.044082</td>\n",
       "      <td>0.322560</td>\n",
       "      <td>0.554672</td>\n",
       "      <td>0.866103</td>\n",
       "      <td>-0.133327</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.359932</td>\n",
       "      <td>0.157166</td>\n",
       "      <td>-0.143854</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.368628</td>\n",
       "      <td>0.144735</td>\n",
       "      <td>-0.124443</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>0.371406</td>\n",
       "      <td>0.147325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952649</td>\n",
       "      <td>0.420258</td>\n",
       "      <td>0.836419</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>0.991499</td>\n",
       "      <td>0.316937</td>\n",
       "      <td>0.865493</td>\n",
       "      <td>-0.152229</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.624891</td>\n",
       "      <td>0.320737</td>\n",
       "      <td>0.094044</td>\n",
       "      <td>0.998914</td>\n",
       "      <td>0.638339</td>\n",
       "      <td>0.311171</td>\n",
       "      <td>0.127390</td>\n",
       "      <td>0.997108</td>\n",
       "      <td>0.640547</td>\n",
       "      <td>0.311827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546328</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>0.747032</td>\n",
       "      <td>-0.183616</td>\n",
       "      <td>0.683150</td>\n",
       "      <td>0.400883</td>\n",
       "      <td>0.735333</td>\n",
       "      <td>-0.288690</td>\n",
       "      <td>0.549176</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.578427</td>\n",
       "      <td>0.293501</td>\n",
       "      <td>0.036197</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.589189</td>\n",
       "      <td>0.282180</td>\n",
       "      <td>0.068370</td>\n",
       "      <td>0.995338</td>\n",
       "      <td>0.591508</td>\n",
       "      <td>0.282465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340686</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.759903</td>\n",
       "      <td>-0.199419</td>\n",
       "      <td>0.554312</td>\n",
       "      <td>0.452171</td>\n",
       "      <td>0.738551</td>\n",
       "      <td>-0.347020</td>\n",
       "      <td>0.439805</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.557951</td>\n",
       "      <td>0.302653</td>\n",
       "      <td>0.051851</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>0.556164</td>\n",
       "      <td>0.289404</td>\n",
       "      <td>0.048522</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.554001</td>\n",
       "      <td>0.287569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952242</td>\n",
       "      <td>0.589816</td>\n",
       "      <td>0.914032</td>\n",
       "      <td>-0.146734</td>\n",
       "      <td>0.971583</td>\n",
       "      <td>0.370397</td>\n",
       "      <td>0.903942</td>\n",
       "      <td>-0.354547</td>\n",
       "      <td>0.977861</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.111426</td>\n",
       "      <td>-0.385205</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.553894</td>\n",
       "      <td>0.088684</td>\n",
       "      <td>-0.387429</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.555849</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924389</td>\n",
       "      <td>0.750969</td>\n",
       "      <td>0.706547</td>\n",
       "      <td>0.037444</td>\n",
       "      <td>0.975823</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>0.755522</td>\n",
       "      <td>0.103371</td>\n",
       "      <td>0.977237</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.873961</td>\n",
       "      <td>0.543320</td>\n",
       "      <td>-0.294131</td>\n",
       "      <td>0.060140</td>\n",
       "      <td>0.888117</td>\n",
       "      <td>0.532091</td>\n",
       "      <td>-0.278737</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.892976</td>\n",
       "      <td>0.521830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>1.866714</td>\n",
       "      <td>2.087578</td>\n",
       "      <td>-0.147916</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>1.718503</td>\n",
       "      <td>2.166074</td>\n",
       "      <td>0.112948</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.558828</td>\n",
       "      <td>0.219975</td>\n",
       "      <td>-1.201636</td>\n",
       "      <td>0.964841</td>\n",
       "      <td>0.503062</td>\n",
       "      <td>0.166280</td>\n",
       "      <td>-1.129104</td>\n",
       "      <td>0.873291</td>\n",
       "      <td>0.508610</td>\n",
       "      <td>0.145475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>3.346029</td>\n",
       "      <td>0.515649</td>\n",
       "      <td>-0.075499</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>3.258759</td>\n",
       "      <td>0.748537</td>\n",
       "      <td>0.440908</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.486904</td>\n",
       "      <td>0.284182</td>\n",
       "      <td>-0.560840</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.492518</td>\n",
       "      <td>0.262894</td>\n",
       "      <td>-0.540399</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>0.495697</td>\n",
       "      <td>0.261363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983780</td>\n",
       "      <td>0.614875</td>\n",
       "      <td>0.927202</td>\n",
       "      <td>-0.111924</td>\n",
       "      <td>0.987525</td>\n",
       "      <td>0.105490</td>\n",
       "      <td>0.898743</td>\n",
       "      <td>-0.419916</td>\n",
       "      <td>0.991762</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.206257</td>\n",
       "      <td>0.537073</td>\n",
       "      <td>-0.441513</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.196335</td>\n",
       "      <td>0.518605</td>\n",
       "      <td>-0.458514</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.199043</td>\n",
       "      <td>0.511683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782706</td>\n",
       "      <td>1.026994</td>\n",
       "      <td>0.260739</td>\n",
       "      <td>-0.584920</td>\n",
       "      <td>0.908104</td>\n",
       "      <td>0.920711</td>\n",
       "      <td>0.792561</td>\n",
       "      <td>-0.543732</td>\n",
       "      <td>0.790173</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.575222</td>\n",
       "      <td>0.305503</td>\n",
       "      <td>-0.565267</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>0.589225</td>\n",
       "      <td>0.290654</td>\n",
       "      <td>-0.575352</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.594806</td>\n",
       "      <td>0.292575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829648</td>\n",
       "      <td>0.768262</td>\n",
       "      <td>0.896095</td>\n",
       "      <td>0.097474</td>\n",
       "      <td>0.234308</td>\n",
       "      <td>0.386894</td>\n",
       "      <td>0.900593</td>\n",
       "      <td>-0.171307</td>\n",
       "      <td>0.716964</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.781515  0.585182 -0.377416  0.999337  0.801634  0.583844 -0.346864   \n",
       "1   0.440957  0.203960 -0.016196  0.999392  0.438722  0.191672 -0.024764   \n",
       "2   0.424696  0.311120 -0.467895  0.992522  0.432865  0.294632 -0.461224   \n",
       "3   0.958737  0.674987 -0.351112  0.301481  0.954889  0.621089 -0.356924   \n",
       "4   0.453867  0.209480 -0.095277  0.999990  0.460642  0.196878 -0.064119   \n",
       "5   0.636110  0.265268  0.008573  0.999501  0.641752  0.253239 -0.021199   \n",
       "6   0.662338  0.296368 -0.555003  0.999995  0.665013  0.278563 -0.550951   \n",
       "7   0.576041  0.288460 -0.224353  0.999851  0.583908  0.278263 -0.200934   \n",
       "8   0.426483  0.285799 -0.365079  0.994168  0.416943  0.275199 -0.352136   \n",
       "9   0.425572  0.303122 -0.525600  0.999820  0.423410  0.280438 -0.505842   \n",
       "10  0.268641  0.454742 -0.234921  0.999194  0.253025  0.448042 -0.218890   \n",
       "11  0.718325  0.406157 -0.038254  0.999238  0.734936  0.403235 -0.033635   \n",
       "12  0.561832  0.290768  0.158343  0.999077  0.559593  0.278882  0.153861   \n",
       "13  0.347349  0.302304 -0.655920  0.999995  0.342640  0.260768 -0.660899   \n",
       "14  0.444452  0.285701 -0.556421  0.999993  0.444904  0.264486 -0.536918   \n",
       "15  0.541733  0.312349 -0.381705  0.999128  0.553890  0.297387 -0.399631   \n",
       "16  0.486044  0.283898 -0.651600  0.999997  0.490568  0.261431 -0.629694   \n",
       "17  0.582448  0.287119 -0.036671  0.999897  0.592861  0.276927 -0.001321   \n",
       "18  0.639713  0.274084 -0.331927  0.999679  0.663728  0.273007 -0.349980   \n",
       "19  0.506971  0.271956  0.013412  0.999683  0.504271  0.256374  0.032579   \n",
       "20  0.359932  0.157166 -0.143854  0.999655  0.368628  0.144735 -0.124443   \n",
       "21  0.624891  0.320737  0.094044  0.998914  0.638339  0.311171  0.127390   \n",
       "22  0.578427  0.293501  0.036197  0.998004  0.589189  0.282180  0.068370   \n",
       "23  0.557951  0.302653  0.051851  0.999802  0.556164  0.289404  0.048522   \n",
       "24  0.559140  0.111426 -0.385205  0.999410  0.553894  0.088684 -0.387429   \n",
       "25  0.873961  0.543320 -0.294131  0.060140  0.888117  0.532091 -0.278737   \n",
       "26  0.558828  0.219975 -1.201636  0.964841  0.503062  0.166280 -1.129104   \n",
       "27  0.486904  0.284182 -0.560840  0.999879  0.492518  0.262894 -0.540399   \n",
       "28  0.206257  0.537073 -0.441513  0.999830  0.196335  0.518605 -0.458514   \n",
       "29  0.575222  0.305503 -0.565267  0.999621  0.589225  0.290654 -0.575352   \n",
       "\n",
       "           7         8         9  ...       123       124       125       126  \\\n",
       "0   0.998953  0.803092  0.584818  ...  0.974083  0.038973  0.727149 -0.345964   \n",
       "1   0.999229  0.439866  0.191195  ...  0.968960  0.474126  0.928752 -0.225557   \n",
       "2   0.988378  0.437681  0.293425  ...  0.820323  0.515408  0.935092 -0.096266   \n",
       "3   0.136248  0.963902  0.608717  ...  0.002587  1.783472  2.337639 -0.143793   \n",
       "4   0.999989  0.464144  0.198346  ...  0.991521  0.433982  0.965759  0.050112   \n",
       "5   0.999582  0.642424  0.252950  ...  0.945748  0.230814  0.697486 -0.023572   \n",
       "6   0.999988  0.666079  0.276262  ...  0.989547  0.697483  0.732398  0.112357   \n",
       "7   0.999721  0.586608  0.279568  ...  0.203585  0.462605  0.748331  0.060704   \n",
       "8   0.993849  0.416726  0.274645  ...  0.972544  0.602495  0.890713 -0.125095   \n",
       "9   0.999863  0.425376  0.278651  ...  0.931002  0.596137  0.890161 -0.111321   \n",
       "10  0.997838  0.252292  0.443633  ...  0.125177  0.736338  0.592920 -0.747410   \n",
       "11  0.997865  0.737823  0.406299  ...  0.449575  0.290581  0.589076 -0.304369   \n",
       "12  0.999107  0.557519  0.277077  ...  0.964355  0.379064  0.903139 -0.242399   \n",
       "13  0.999990  0.349008  0.253441  ...  0.882690  0.929926  0.550429 -0.217673   \n",
       "14  0.999995  0.447445  0.263364  ...  0.987457  0.563189  0.915141 -0.121690   \n",
       "15  0.998945  0.558469  0.298456  ...  0.745611  0.804409  0.830813  0.255628   \n",
       "16  0.999997  0.494000  0.259873  ...  0.995542  0.606093  0.920141 -0.060707   \n",
       "17  0.999824  0.595638  0.277430  ...  0.789295  0.453279  0.733115  0.044083   \n",
       "18  0.999599  0.669219  0.278887  ...  0.707585  0.430985  1.039195 -0.353327   \n",
       "19  0.999572  0.509093  0.253745  ...  0.298918  0.557836  0.870850  0.044082   \n",
       "20  0.999307  0.371406  0.147325  ...  0.952649  0.420258  0.836419  0.047267   \n",
       "21  0.997108  0.640547  0.311827  ...  0.546328  0.429357  0.747032 -0.183616   \n",
       "22  0.995338  0.591508  0.282465  ...  0.340686  0.487500  0.759903 -0.199419   \n",
       "23  0.999707  0.554001  0.287569  ...  0.952242  0.589816  0.914032 -0.146734   \n",
       "24  0.999203  0.555849  0.082484  ...  0.924389  0.750969  0.706547  0.037444   \n",
       "25  0.023264  0.892976  0.521830  ...  0.001404  1.866714  2.087578 -0.147916   \n",
       "26  0.873291  0.508610  0.145475  ...  0.003882  3.346029  0.515649 -0.075499   \n",
       "27  0.999858  0.495697  0.261363  ...  0.983780  0.614875  0.927202 -0.111924   \n",
       "28  0.999746  0.199043  0.511683  ...  0.782706  1.026994  0.260739 -0.584920   \n",
       "29  0.999521  0.594806  0.292575  ...  0.829648  0.768262  0.896095  0.097474   \n",
       "\n",
       "         127       128       129       130       131  labels  \n",
       "0   0.984270  0.039695  0.140549 -0.396382  0.992604   drive  \n",
       "1   0.982506  0.705118  0.875454  0.124557  0.955982   drive  \n",
       "2   0.941868  0.360824  0.932174 -0.283494  0.962177   drive  \n",
       "3   0.002774  1.531482  2.318639 -0.034189  0.003036   drive  \n",
       "4   0.941402  0.664213  0.864891 -0.348769  0.990185   drive  \n",
       "5   0.969140  0.393944  0.942258  0.144790  0.890441   drive  \n",
       "6   0.975039  0.166634  0.761494 -0.307279  0.998335   drive  \n",
       "7   0.272436  0.477370  0.754690 -0.168187  0.341884   drive  \n",
       "8   0.993906  0.229585  0.888474 -0.097694  0.987159   drive  \n",
       "9   0.984413  0.226039  0.890382  0.068088  0.953154   drive  \n",
       "10  0.156130  0.727507  0.603030 -0.488235  0.148493   drive  \n",
       "11  0.200060  0.228219  0.519589 -0.377512  0.641665   drive  \n",
       "12  0.960533  0.590812  0.913470 -0.190760  0.879906   drive  \n",
       "13  0.979205  0.514133  1.053639 -0.407969  0.938037   drive  \n",
       "14  0.997384  0.190810  0.825085 -0.059542  0.995471   drive  \n",
       "15  0.284838  0.402060  0.916217 -0.304885  0.750260   drive  \n",
       "16  0.998545  0.104275  0.907561 -0.388223  0.998727   drive  \n",
       "17  0.561363  0.480221  0.747601 -0.208855  0.760426   drive  \n",
       "18  0.703965 -0.035826  0.653020 -0.390210  0.649675   drive  \n",
       "19  0.322560  0.554672  0.866103 -0.133327  0.442264   drive  \n",
       "20  0.991499  0.316937  0.865493 -0.152229  0.993022   drive  \n",
       "21  0.683150  0.400883  0.735333 -0.288690  0.549176   drive  \n",
       "22  0.554312  0.452171  0.738551 -0.347020  0.439805   drive  \n",
       "23  0.971583  0.370397  0.903942 -0.354547  0.977861   drive  \n",
       "24  0.975823  0.652192  0.755522  0.103371  0.977237   drive  \n",
       "25  0.001348  1.718503  2.166074  0.112948  0.000963   drive  \n",
       "26  0.004614  3.258759  0.748537  0.440908  0.002700   drive  \n",
       "27  0.987525  0.105490  0.898743 -0.419916  0.991762   drive  \n",
       "28  0.908104  0.920711  0.792561 -0.543732  0.790173   drive  \n",
       "29  0.234308  0.386894  0.900593 -0.171307  0.716964   drive  \n",
       "\n",
       "[30 rows x 133 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./poses.csv')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymu_XWZst9PY",
    "outputId": "782903df-efd1-433c-91fa-a4af6337d511"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['drive', 'legglance-flick', 'sweep', 'pullshot'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eovx_qcMuDAW"
   },
   "outputs": [],
   "source": [
    "mapping = {'drive':0,'legglance-flick':1,'sweep':2,'pullshot':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wfzmfAK6yIrE"
   },
   "outputs": [],
   "source": [
    "df1 = df.replace({'labels':mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "e5Wz01yLyPG2",
    "outputId": "7871dfcb-fa2d-46a0-8198-92a259c65686"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781515</td>\n",
       "      <td>0.585182</td>\n",
       "      <td>-0.377416</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.801634</td>\n",
       "      <td>0.583844</td>\n",
       "      <td>-0.346864</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.803092</td>\n",
       "      <td>0.584818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974083</td>\n",
       "      <td>0.038973</td>\n",
       "      <td>0.727149</td>\n",
       "      <td>-0.345964</td>\n",
       "      <td>0.984270</td>\n",
       "      <td>0.039695</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>-0.396382</td>\n",
       "      <td>0.992604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440957</td>\n",
       "      <td>0.203960</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.438722</td>\n",
       "      <td>0.191672</td>\n",
       "      <td>-0.024764</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.439866</td>\n",
       "      <td>0.191195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.474126</td>\n",
       "      <td>0.928752</td>\n",
       "      <td>-0.225557</td>\n",
       "      <td>0.982506</td>\n",
       "      <td>0.705118</td>\n",
       "      <td>0.875454</td>\n",
       "      <td>0.124557</td>\n",
       "      <td>0.955982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.424696</td>\n",
       "      <td>0.311120</td>\n",
       "      <td>-0.467895</td>\n",
       "      <td>0.992522</td>\n",
       "      <td>0.432865</td>\n",
       "      <td>0.294632</td>\n",
       "      <td>-0.461224</td>\n",
       "      <td>0.988378</td>\n",
       "      <td>0.437681</td>\n",
       "      <td>0.293425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820323</td>\n",
       "      <td>0.515408</td>\n",
       "      <td>0.935092</td>\n",
       "      <td>-0.096266</td>\n",
       "      <td>0.941868</td>\n",
       "      <td>0.360824</td>\n",
       "      <td>0.932174</td>\n",
       "      <td>-0.283494</td>\n",
       "      <td>0.962177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958737</td>\n",
       "      <td>0.674987</td>\n",
       "      <td>-0.351112</td>\n",
       "      <td>0.301481</td>\n",
       "      <td>0.954889</td>\n",
       "      <td>0.621089</td>\n",
       "      <td>-0.356924</td>\n",
       "      <td>0.136248</td>\n",
       "      <td>0.963902</td>\n",
       "      <td>0.608717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>1.783472</td>\n",
       "      <td>2.337639</td>\n",
       "      <td>-0.143793</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>1.531482</td>\n",
       "      <td>2.318639</td>\n",
       "      <td>-0.034189</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.453867</td>\n",
       "      <td>0.209480</td>\n",
       "      <td>-0.095277</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.460642</td>\n",
       "      <td>0.196878</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.464144</td>\n",
       "      <td>0.198346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991521</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.965759</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.941402</td>\n",
       "      <td>0.664213</td>\n",
       "      <td>0.864891</td>\n",
       "      <td>-0.348769</td>\n",
       "      <td>0.990185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.781515  0.585182 -0.377416  0.999337  0.801634  0.583844 -0.346864   \n",
       "1  0.440957  0.203960 -0.016196  0.999392  0.438722  0.191672 -0.024764   \n",
       "2  0.424696  0.311120 -0.467895  0.992522  0.432865  0.294632 -0.461224   \n",
       "3  0.958737  0.674987 -0.351112  0.301481  0.954889  0.621089 -0.356924   \n",
       "4  0.453867  0.209480 -0.095277  0.999990  0.460642  0.196878 -0.064119   \n",
       "\n",
       "          7         8         9  ...       123       124       125       126  \\\n",
       "0  0.998953  0.803092  0.584818  ...  0.974083  0.038973  0.727149 -0.345964   \n",
       "1  0.999229  0.439866  0.191195  ...  0.968960  0.474126  0.928752 -0.225557   \n",
       "2  0.988378  0.437681  0.293425  ...  0.820323  0.515408  0.935092 -0.096266   \n",
       "3  0.136248  0.963902  0.608717  ...  0.002587  1.783472  2.337639 -0.143793   \n",
       "4  0.999989  0.464144  0.198346  ...  0.991521  0.433982  0.965759  0.050112   \n",
       "\n",
       "        127       128       129       130       131  labels  \n",
       "0  0.984270  0.039695  0.140549 -0.396382  0.992604       0  \n",
       "1  0.982506  0.705118  0.875454  0.124557  0.955982       0  \n",
       "2  0.941868  0.360824  0.932174 -0.283494  0.962177       0  \n",
       "3  0.002774  1.531482  2.318639 -0.034189  0.003036       0  \n",
       "4  0.941402  0.664213  0.864891 -0.348769  0.990185       0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gos6UQxNygBs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "riJKzdDRy3yF"
   },
   "outputs": [],
   "source": [
    "x = df1.drop('labels',axis=1)\n",
    "y = df1['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "PO2wGXb3zFfj",
    "outputId": "bf226646-9152-4b0c-fd9b-50f7cf0ea818"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781515</td>\n",
       "      <td>0.585182</td>\n",
       "      <td>-0.377416</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.801634</td>\n",
       "      <td>0.583844</td>\n",
       "      <td>-0.346864</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.803092</td>\n",
       "      <td>0.584818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226036</td>\n",
       "      <td>0.974083</td>\n",
       "      <td>0.038973</td>\n",
       "      <td>0.727149</td>\n",
       "      <td>-0.345964</td>\n",
       "      <td>0.984270</td>\n",
       "      <td>0.039695</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>-0.396382</td>\n",
       "      <td>0.992604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440957</td>\n",
       "      <td>0.203960</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.438722</td>\n",
       "      <td>0.191672</td>\n",
       "      <td>-0.024764</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.439866</td>\n",
       "      <td>0.191195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182208</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.474126</td>\n",
       "      <td>0.928752</td>\n",
       "      <td>-0.225557</td>\n",
       "      <td>0.982506</td>\n",
       "      <td>0.705118</td>\n",
       "      <td>0.875454</td>\n",
       "      <td>0.124557</td>\n",
       "      <td>0.955982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.424696</td>\n",
       "      <td>0.311120</td>\n",
       "      <td>-0.467895</td>\n",
       "      <td>0.992522</td>\n",
       "      <td>0.432865</td>\n",
       "      <td>0.294632</td>\n",
       "      <td>-0.461224</td>\n",
       "      <td>0.988378</td>\n",
       "      <td>0.437681</td>\n",
       "      <td>0.293425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126770</td>\n",
       "      <td>0.820323</td>\n",
       "      <td>0.515408</td>\n",
       "      <td>0.935092</td>\n",
       "      <td>-0.096266</td>\n",
       "      <td>0.941868</td>\n",
       "      <td>0.360824</td>\n",
       "      <td>0.932174</td>\n",
       "      <td>-0.283494</td>\n",
       "      <td>0.962177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958737</td>\n",
       "      <td>0.674987</td>\n",
       "      <td>-0.351112</td>\n",
       "      <td>0.301481</td>\n",
       "      <td>0.954889</td>\n",
       "      <td>0.621089</td>\n",
       "      <td>-0.356924</td>\n",
       "      <td>0.136248</td>\n",
       "      <td>0.963902</td>\n",
       "      <td>0.608717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235142</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>1.783472</td>\n",
       "      <td>2.337639</td>\n",
       "      <td>-0.143793</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>1.531482</td>\n",
       "      <td>2.318639</td>\n",
       "      <td>-0.034189</td>\n",
       "      <td>0.003036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.453867</td>\n",
       "      <td>0.209480</td>\n",
       "      <td>-0.095277</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.460642</td>\n",
       "      <td>0.196878</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.464144</td>\n",
       "      <td>0.198346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222206</td>\n",
       "      <td>0.991521</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.965759</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.941402</td>\n",
       "      <td>0.664213</td>\n",
       "      <td>0.864891</td>\n",
       "      <td>-0.348769</td>\n",
       "      <td>0.990185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>0.431821</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.099769</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.438675</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255784</td>\n",
       "      <td>0.964661</td>\n",
       "      <td>0.266845</td>\n",
       "      <td>0.852026</td>\n",
       "      <td>-0.308086</td>\n",
       "      <td>0.995516</td>\n",
       "      <td>0.464124</td>\n",
       "      <td>0.896388</td>\n",
       "      <td>0.181759</td>\n",
       "      <td>0.960530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>0.578297</td>\n",
       "      <td>0.122530</td>\n",
       "      <td>-0.045721</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.588502</td>\n",
       "      <td>0.105419</td>\n",
       "      <td>-0.024728</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.594113</td>\n",
       "      <td>0.106804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173878</td>\n",
       "      <td>0.995546</td>\n",
       "      <td>0.624668</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>-0.124259</td>\n",
       "      <td>0.994733</td>\n",
       "      <td>0.688607</td>\n",
       "      <td>0.809072</td>\n",
       "      <td>-0.318697</td>\n",
       "      <td>0.998613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>0.428360</td>\n",
       "      <td>0.162388</td>\n",
       "      <td>-0.004118</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.434751</td>\n",
       "      <td>0.145803</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.436093</td>\n",
       "      <td>0.145955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134417</td>\n",
       "      <td>0.999533</td>\n",
       "      <td>0.156619</td>\n",
       "      <td>0.870368</td>\n",
       "      <td>-0.124812</td>\n",
       "      <td>0.994151</td>\n",
       "      <td>0.544078</td>\n",
       "      <td>0.892828</td>\n",
       "      <td>-0.178402</td>\n",
       "      <td>0.998913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>0.702476</td>\n",
       "      <td>0.330995</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.716885</td>\n",
       "      <td>0.333530</td>\n",
       "      <td>0.038263</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.719056</td>\n",
       "      <td>0.338493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012314</td>\n",
       "      <td>0.958729</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.588366</td>\n",
       "      <td>-0.302583</td>\n",
       "      <td>0.959395</td>\n",
       "      <td>0.299827</td>\n",
       "      <td>0.863043</td>\n",
       "      <td>-0.070109</td>\n",
       "      <td>0.895234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>0.739734</td>\n",
       "      <td>0.258002</td>\n",
       "      <td>-0.340665</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.760058</td>\n",
       "      <td>0.247701</td>\n",
       "      <td>-0.350763</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.767096</td>\n",
       "      <td>0.250199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331557</td>\n",
       "      <td>0.581313</td>\n",
       "      <td>0.728129</td>\n",
       "      <td>1.027094</td>\n",
       "      <td>-0.204077</td>\n",
       "      <td>0.647828</td>\n",
       "      <td>0.635330</td>\n",
       "      <td>0.976736</td>\n",
       "      <td>-0.602802</td>\n",
       "      <td>0.859581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3687 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.781515  0.585182 -0.377416  0.999337  0.801634  0.583844 -0.346864   \n",
       "1     0.440957  0.203960 -0.016196  0.999392  0.438722  0.191672 -0.024764   \n",
       "2     0.424696  0.311120 -0.467895  0.992522  0.432865  0.294632 -0.461224   \n",
       "3     0.958737  0.674987 -0.351112  0.301481  0.954889  0.621089 -0.356924   \n",
       "4     0.453867  0.209480 -0.095277  0.999990  0.460642  0.196878 -0.064119   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3682  0.431821  0.119368  0.023989  1.000000  0.435001  0.099769  0.049860   \n",
       "3683  0.578297  0.122530 -0.045721  0.999998  0.588502  0.105419 -0.024728   \n",
       "3684  0.428360  0.162388 -0.004118  0.999706  0.434751  0.145803 -0.007627   \n",
       "3685  0.702476  0.330995  0.055684  0.999851  0.716885  0.333530  0.038263   \n",
       "3686  0.739734  0.258002 -0.340665  0.999225  0.760058  0.247701 -0.350763   \n",
       "\n",
       "             7         8         9  ...       122       123       124  \\\n",
       "0     0.998953  0.803092  0.584818  ... -0.226036  0.974083  0.038973   \n",
       "1     0.999229  0.439866  0.191195  ...  0.182208  0.968960  0.474126   \n",
       "2     0.988378  0.437681  0.293425  ... -0.126770  0.820323  0.515408   \n",
       "3     0.136248  0.963902  0.608717  ...  0.235142  0.002587  1.783472   \n",
       "4     0.999989  0.464144  0.198346  ... -0.222206  0.991521  0.433982   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3682  1.000000  0.438675  0.099291  ...  0.255784  0.964661  0.266845   \n",
       "3683  0.999997  0.594113  0.106804  ... -0.173878  0.995546  0.624668   \n",
       "3684  0.999800  0.436093  0.145955  ... -0.134417  0.999533  0.156619   \n",
       "3685  0.999915  0.719056  0.338493  ... -0.012314  0.958729  0.150741   \n",
       "3686  0.999244  0.767096  0.250199  ... -0.331557  0.581313  0.728129   \n",
       "\n",
       "           125       126       127       128       129       130       131  \n",
       "0     0.727149 -0.345964  0.984270  0.039695  0.140549 -0.396382  0.992604  \n",
       "1     0.928752 -0.225557  0.982506  0.705118  0.875454  0.124557  0.955982  \n",
       "2     0.935092 -0.096266  0.941868  0.360824  0.932174 -0.283494  0.962177  \n",
       "3     2.337639 -0.143793  0.002774  1.531482  2.318639 -0.034189  0.003036  \n",
       "4     0.965759  0.050112  0.941402  0.664213  0.864891 -0.348769  0.990185  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3682  0.852026 -0.308086  0.995516  0.464124  0.896388  0.181759  0.960530  \n",
       "3683  0.844622 -0.124259  0.994733  0.688607  0.809072 -0.318697  0.998613  \n",
       "3684  0.870368 -0.124812  0.994151  0.544078  0.892828 -0.178402  0.998913  \n",
       "3685  0.588366 -0.302583  0.959395  0.299827  0.863043 -0.070109  0.895234  \n",
       "3686  1.027094 -0.204077  0.647828  0.635330  0.976736 -0.602802  0.859581  \n",
       "\n",
       "[3687 rows x 132 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2lHfh2GzLQM",
    "outputId": "660a9ca4-a785-4ec8-86c8-444789203900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3682    3\n",
       "3683    3\n",
       "3684    3\n",
       "3685    3\n",
       "3686    3\n",
       "Name: labels, Length: 3687, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_rqt14H0zOMR"
   },
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxM_W0IOznN0",
    "outputId": "22eddbc9-c000-4120-c749-269f444dd414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2949, 132)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpI437R1zpsW",
    "outputId": "30969a70-d02c-4a81-afcb-88ec89e727da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(738, 132)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OHOSpi2-KNah"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9JQb1pEwzsxL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "                                    Dense(24,activation='relu',input_shape = (132,)),\n",
    "                                    Dense(48,activation='relu'),\n",
    "                                    Dense(24,activation='relu'),\n",
    "                                    Dense(4,activation='softmax')\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwMeSMw40Qwt",
    "outputId": "e0ceaa4d-de05-4ed4-cc63-4ca4aec25057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                3192      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 48)                1200      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                1176      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,668\n",
      "Trainable params: 5,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3vnqrK7D0i5v"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wn3yuVSeOXvi",
    "outputId": "714bef67-85e2-4a0c-de51-bf6c7abb0519",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.2687 - accuracy: 0.4484INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 3s 17ms/step - loss: 1.2687 - accuracy: 0.4484 - val_loss: 1.1521 - val_accuracy: 0.5153\n",
      "Epoch 2/250\n",
      "82/83 [============================>.] - ETA: 0s - loss: 1.0578 - accuracy: 0.5877INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 1.0574 - accuracy: 0.5874 - val_loss: 0.9364 - val_accuracy: 0.6542\n",
      "Epoch 3/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9366 - accuracy: 0.6213 - val_loss: 0.8610 - val_accuracy: 0.6542\n",
      "Epoch 4/250\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8815 - accuracy: 0.6402INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.8815 - accuracy: 0.6402 - val_loss: 0.8446 - val_accuracy: 0.6915\n",
      "Epoch 5/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.8572 - accuracy: 0.6515 - val_loss: 0.7873 - val_accuracy: 0.6915\n",
      "Epoch 6/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.8160 - accuracy: 0.6756 - val_loss: 0.7871 - val_accuracy: 0.6678\n",
      "Epoch 7/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.7942 - accuracy: 0.6820 - val_loss: 0.7285 - val_accuracy: 0.6780\n",
      "Epoch 8/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.7682 - accuracy: 0.6925 - val_loss: 0.7550 - val_accuracy: 0.6847\n",
      "Epoch 9/250\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.7310 - accuracy: 0.7195INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.7290 - accuracy: 0.7212 - val_loss: 0.7060 - val_accuracy: 0.7153\n",
      "Epoch 10/250\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.7236 - accuracy: 0.7066INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.7237 - accuracy: 0.7072 - val_loss: 0.6674 - val_accuracy: 0.7288\n",
      "Epoch 11/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.6827 - accuracy: 0.7265 - val_loss: 0.6567 - val_accuracy: 0.7254\n",
      "Epoch 12/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.6723 - accuracy: 0.7378 - val_loss: 0.6474 - val_accuracy: 0.7220\n",
      "Epoch 13/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.6431 - accuracy: 0.7468 - val_loss: 0.6572 - val_accuracy: 0.7153\n",
      "Epoch 14/250\n",
      "77/83 [==========================>...] - ETA: 0s - loss: 0.6232 - accuracy: 0.7569INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.6280 - accuracy: 0.7528 - val_loss: 0.6156 - val_accuracy: 0.7593\n",
      "Epoch 15/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.6035 - accuracy: 0.7705 - val_loss: 0.5960 - val_accuracy: 0.7356\n",
      "Epoch 16/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.5835 - accuracy: 0.7773 - val_loss: 0.5749 - val_accuracy: 0.7559\n",
      "Epoch 17/250\n",
      "79/83 [===========================>..] - ETA: 0s - loss: 0.5794 - accuracy: 0.7824INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.5782 - accuracy: 0.7818 - val_loss: 0.5515 - val_accuracy: 0.7763\n",
      "Epoch 18/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.5628 - accuracy: 0.7875 - val_loss: 0.5801 - val_accuracy: 0.7627\n",
      "Epoch 19/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.5456 - accuracy: 0.7871 - val_loss: 0.5697 - val_accuracy: 0.7661\n",
      "Epoch 20/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.5357 - accuracy: 0.7943 - val_loss: 0.5573 - val_accuracy: 0.7661\n",
      "Epoch 21/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.5310 - accuracy: 0.7935 - val_loss: 0.5610 - val_accuracy: 0.7729\n",
      "Epoch 22/250\n",
      "80/83 [===========================>..] - ETA: 0s - loss: 0.5195 - accuracy: 0.7992INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.5255 - accuracy: 0.7973 - val_loss: 0.5096 - val_accuracy: 0.7797\n",
      "Epoch 23/250\n",
      "80/83 [===========================>..] - ETA: 0s - loss: 0.4935 - accuracy: 0.8098INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.4933 - accuracy: 0.8109 - val_loss: 0.5069 - val_accuracy: 0.7864\n",
      "Epoch 24/250\n",
      "80/83 [===========================>..] - ETA: 0s - loss: 0.5195 - accuracy: 0.8020INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.5183 - accuracy: 0.8041 - val_loss: 0.5090 - val_accuracy: 0.7932\n",
      "Epoch 25/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4990 - accuracy: 0.8063 - val_loss: 0.4897 - val_accuracy: 0.7831\n",
      "Epoch 26/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4724 - accuracy: 0.8191 - val_loss: 0.5192 - val_accuracy: 0.7932\n",
      "Epoch 27/250\n",
      "80/83 [===========================>..] - ETA: 0s - loss: 0.4549 - accuracy: 0.8297INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.4537 - accuracy: 0.8297 - val_loss: 0.4921 - val_accuracy: 0.8034\n",
      "Epoch 28/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4450 - accuracy: 0.8365 - val_loss: 0.4989 - val_accuracy: 0.7831\n",
      "Epoch 29/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4459 - accuracy: 0.8308 - val_loss: 0.4823 - val_accuracy: 0.7898\n",
      "Epoch 30/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.4404 - accuracy: 0.8350 - val_loss: 0.5027 - val_accuracy: 0.7864\n",
      "Epoch 31/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.4184 - accuracy: 0.8459 - val_loss: 0.4784 - val_accuracy: 0.7831\n",
      "Epoch 32/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.4226 - accuracy: 0.8459 - val_loss: 0.4882 - val_accuracy: 0.7898\n",
      "Epoch 33/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.4173 - accuracy: 0.8414 - val_loss: 0.5372 - val_accuracy: 0.7864\n",
      "Epoch 34/250\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8451INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.4139 - accuracy: 0.8451 - val_loss: 0.4656 - val_accuracy: 0.8102\n",
      "Epoch 35/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4005 - accuracy: 0.8485 - val_loss: 0.4581 - val_accuracy: 0.8102\n",
      "Epoch 36/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3925 - accuracy: 0.8493 - val_loss: 0.4627 - val_accuracy: 0.7864\n",
      "Epoch 37/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4003 - accuracy: 0.8451 - val_loss: 0.5400 - val_accuracy: 0.7797\n",
      "Epoch 38/250\n",
      "78/83 [===========================>..] - ETA: 0s - loss: 0.3872 - accuracy: 0.8562INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.3827 - accuracy: 0.8587 - val_loss: 0.4704 - val_accuracy: 0.8271\n",
      "Epoch 39/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3768 - accuracy: 0.8576 - val_loss: 0.4403 - val_accuracy: 0.8136\n",
      "Epoch 40/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3752 - accuracy: 0.8583 - val_loss: 0.4634 - val_accuracy: 0.8237\n",
      "Epoch 41/250\n",
      "77/83 [==========================>...] - ETA: 0s - loss: 0.3630 - accuracy: 0.8628INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.3698 - accuracy: 0.8587 - val_loss: 0.4356 - val_accuracy: 0.8475\n",
      "Epoch 42/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3493 - accuracy: 0.8749 - val_loss: 0.4404 - val_accuracy: 0.8305\n",
      "Epoch 43/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3483 - accuracy: 0.8708 - val_loss: 0.4228 - val_accuracy: 0.8339\n",
      "Epoch 44/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3601 - accuracy: 0.8632 - val_loss: 0.4868 - val_accuracy: 0.8305\n",
      "Epoch 45/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3414 - accuracy: 0.8704 - val_loss: 0.4637 - val_accuracy: 0.7932\n",
      "Epoch 46/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3371 - accuracy: 0.8775 - val_loss: 0.5037 - val_accuracy: 0.8203\n",
      "Epoch 47/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3345 - accuracy: 0.8764 - val_loss: 0.4168 - val_accuracy: 0.8339\n",
      "Epoch 48/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3175 - accuracy: 0.8817 - val_loss: 0.4175 - val_accuracy: 0.8407\n",
      "Epoch 49/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3113 - accuracy: 0.8787 - val_loss: 0.4216 - val_accuracy: 0.8305\n",
      "Epoch 50/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.3255 - accuracy: 0.8791 - val_loss: 0.4491 - val_accuracy: 0.8102\n",
      "Epoch 51/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3228 - accuracy: 0.8768 - val_loss: 0.4843 - val_accuracy: 0.8102\n",
      "Epoch 52/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.3164 - accuracy: 0.8828 - val_loss: 0.4944 - val_accuracy: 0.8068\n",
      "Epoch 53/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2971 - accuracy: 0.8904 - val_loss: 0.3880 - val_accuracy: 0.8373\n",
      "Epoch 54/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2978 - accuracy: 0.8866 - val_loss: 0.4034 - val_accuracy: 0.8305\n",
      "Epoch 55/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2908 - accuracy: 0.8900 - val_loss: 0.4099 - val_accuracy: 0.8305\n",
      "Epoch 56/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2873 - accuracy: 0.8941 - val_loss: 0.4238 - val_accuracy: 0.8237\n",
      "Epoch 57/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2753 - accuracy: 0.8971 - val_loss: 0.4193 - val_accuracy: 0.8441\n",
      "Epoch 58/250\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.9024INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 0.2691 - accuracy: 0.9017 - val_loss: 0.4183 - val_accuracy: 0.8610\n",
      "Epoch 59/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2855 - accuracy: 0.8922 - val_loss: 0.4059 - val_accuracy: 0.8373\n",
      "Epoch 60/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2770 - accuracy: 0.8994 - val_loss: 0.4153 - val_accuracy: 0.8373\n",
      "Epoch 61/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.2606 - accuracy: 0.9077 - val_loss: 0.4740 - val_accuracy: 0.8169\n",
      "Epoch 62/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2748 - accuracy: 0.8979 - val_loss: 0.4128 - val_accuracy: 0.8407\n",
      "Epoch 63/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2712 - accuracy: 0.8945 - val_loss: 0.4116 - val_accuracy: 0.8305\n",
      "Epoch 64/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2640 - accuracy: 0.9058 - val_loss: 0.4309 - val_accuracy: 0.8237\n",
      "Epoch 65/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2559 - accuracy: 0.9069 - val_loss: 0.3882 - val_accuracy: 0.8508\n",
      "Epoch 66/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2352 - accuracy: 0.9141 - val_loss: 0.4310 - val_accuracy: 0.8271\n",
      "Epoch 67/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2396 - accuracy: 0.9148 - val_loss: 0.4048 - val_accuracy: 0.8508\n",
      "Epoch 68/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2414 - accuracy: 0.9084 - val_loss: 0.4227 - val_accuracy: 0.8407\n",
      "Epoch 69/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.2367 - accuracy: 0.9160 - val_loss: 0.3884 - val_accuracy: 0.8508\n",
      "Epoch 70/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2489 - accuracy: 0.9077 - val_loss: 0.4532 - val_accuracy: 0.8237\n",
      "Epoch 71/250\n",
      "79/83 [===========================>..] - ETA: 0s - loss: 0.2246 - accuracy: 0.9197INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.2268 - accuracy: 0.9179 - val_loss: 0.3939 - val_accuracy: 0.8746\n",
      "Epoch 72/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2504 - accuracy: 0.9096 - val_loss: 0.4051 - val_accuracy: 0.8339\n",
      "Epoch 73/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2257 - accuracy: 0.9171 - val_loss: 0.4045 - val_accuracy: 0.8542\n",
      "Epoch 74/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2126 - accuracy: 0.9190 - val_loss: 0.4334 - val_accuracy: 0.8169\n",
      "Epoch 75/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2173 - accuracy: 0.9213 - val_loss: 0.3972 - val_accuracy: 0.8475\n",
      "Epoch 76/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2270 - accuracy: 0.9239 - val_loss: 0.4735 - val_accuracy: 0.8475\n",
      "Epoch 77/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.2091 - accuracy: 0.9258 - val_loss: 0.3928 - val_accuracy: 0.8576\n",
      "Epoch 78/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2107 - accuracy: 0.9258 - val_loss: 0.4228 - val_accuracy: 0.8339\n",
      "Epoch 79/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2022 - accuracy: 0.9337 - val_loss: 0.5866 - val_accuracy: 0.7797\n",
      "Epoch 80/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2198 - accuracy: 0.9171 - val_loss: 0.3855 - val_accuracy: 0.8610\n",
      "Epoch 81/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2091 - accuracy: 0.9269 - val_loss: 0.4051 - val_accuracy: 0.8542\n",
      "Epoch 82/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9314 - val_loss: 0.3777 - val_accuracy: 0.8610\n",
      "Epoch 83/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2050 - accuracy: 0.9292 - val_loss: 0.3844 - val_accuracy: 0.8576\n",
      "Epoch 84/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9269 - val_loss: 0.4285 - val_accuracy: 0.8542\n",
      "Epoch 85/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1997 - accuracy: 0.9310 - val_loss: 0.4240 - val_accuracy: 0.8508\n",
      "Epoch 86/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1803 - accuracy: 0.9382 - val_loss: 0.4535 - val_accuracy: 0.8441\n",
      "Epoch 87/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2127 - accuracy: 0.9171 - val_loss: 0.4410 - val_accuracy: 0.8610\n",
      "Epoch 88/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1899 - accuracy: 0.9386 - val_loss: 0.4205 - val_accuracy: 0.8576\n",
      "Epoch 89/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1881 - accuracy: 0.9318 - val_loss: 0.4345 - val_accuracy: 0.8339\n",
      "Epoch 90/250\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9337INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.1777 - accuracy: 0.9344 - val_loss: 0.4142 - val_accuracy: 0.8780\n",
      "Epoch 91/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1750 - accuracy: 0.9390 - val_loss: 0.3719 - val_accuracy: 0.8644\n",
      "Epoch 92/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1798 - accuracy: 0.9344 - val_loss: 0.4370 - val_accuracy: 0.8508\n",
      "Epoch 93/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1704 - accuracy: 0.9386 - val_loss: 0.4743 - val_accuracy: 0.8339\n",
      "Epoch 94/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1827 - accuracy: 0.9344 - val_loss: 0.4232 - val_accuracy: 0.8576\n",
      "Epoch 95/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1644 - accuracy: 0.9408 - val_loss: 0.4538 - val_accuracy: 0.8542\n",
      "Epoch 96/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1693 - accuracy: 0.9424 - val_loss: 0.4206 - val_accuracy: 0.8475\n",
      "Epoch 97/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1649 - accuracy: 0.9367 - val_loss: 0.4101 - val_accuracy: 0.8610\n",
      "Epoch 98/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1706 - accuracy: 0.9416 - val_loss: 0.4063 - val_accuracy: 0.8780\n",
      "Epoch 99/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1646 - accuracy: 0.9390 - val_loss: 0.4667 - val_accuracy: 0.8508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1565 - accuracy: 0.9397 - val_loss: 0.4073 - val_accuracy: 0.8508\n",
      "Epoch 101/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1510 - accuracy: 0.9495 - val_loss: 0.4236 - val_accuracy: 0.8678\n",
      "Epoch 102/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1584 - accuracy: 0.9446 - val_loss: 0.5413 - val_accuracy: 0.8339\n",
      "Epoch 103/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1642 - accuracy: 0.9450 - val_loss: 0.4385 - val_accuracy: 0.8475\n",
      "Epoch 104/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1612 - accuracy: 0.9412 - val_loss: 0.4606 - val_accuracy: 0.8271\n",
      "Epoch 105/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1779 - accuracy: 0.9322 - val_loss: 0.4674 - val_accuracy: 0.8339\n",
      "Epoch 106/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1548 - accuracy: 0.9469 - val_loss: 0.4070 - val_accuracy: 0.8678\n",
      "Epoch 107/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1522 - accuracy: 0.9472 - val_loss: 0.4559 - val_accuracy: 0.8441\n",
      "Epoch 108/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1655 - accuracy: 0.9367 - val_loss: 0.4622 - val_accuracy: 0.8542\n",
      "Epoch 109/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1392 - accuracy: 0.9503 - val_loss: 0.5813 - val_accuracy: 0.8237\n",
      "Epoch 110/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1507 - accuracy: 0.9488 - val_loss: 0.4436 - val_accuracy: 0.8644\n",
      "Epoch 111/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1440 - accuracy: 0.9525 - val_loss: 0.4848 - val_accuracy: 0.8576\n",
      "Epoch 112/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1579 - accuracy: 0.9442 - val_loss: 0.4359 - val_accuracy: 0.8576\n",
      "Epoch 113/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1494 - accuracy: 0.9439 - val_loss: 0.4345 - val_accuracy: 0.8678\n",
      "Epoch 114/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1272 - accuracy: 0.9578 - val_loss: 0.4683 - val_accuracy: 0.8610\n",
      "Epoch 115/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1335 - accuracy: 0.9533 - val_loss: 0.4626 - val_accuracy: 0.8576\n",
      "Epoch 116/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1413 - accuracy: 0.9465 - val_loss: 0.4383 - val_accuracy: 0.8678\n",
      "Epoch 117/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1312 - accuracy: 0.9567 - val_loss: 0.4772 - val_accuracy: 0.8508\n",
      "Epoch 118/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1549 - accuracy: 0.9442 - val_loss: 0.5222 - val_accuracy: 0.8508\n",
      "Epoch 119/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1297 - accuracy: 0.9574 - val_loss: 0.4700 - val_accuracy: 0.8542\n",
      "Epoch 120/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1405 - accuracy: 0.9469 - val_loss: 0.4934 - val_accuracy: 0.8576\n",
      "Epoch 121/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1406 - accuracy: 0.9488 - val_loss: 0.4657 - val_accuracy: 0.8644\n",
      "Epoch 122/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1362 - accuracy: 0.9529 - val_loss: 0.4322 - val_accuracy: 0.8576\n",
      "Epoch 123/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1234 - accuracy: 0.9559 - val_loss: 0.5068 - val_accuracy: 0.8475\n",
      "Epoch 124/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1120 - accuracy: 0.9631 - val_loss: 0.4238 - val_accuracy: 0.8712\n",
      "Epoch 125/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1344 - accuracy: 0.9518 - val_loss: 0.5446 - val_accuracy: 0.8305\n",
      "Epoch 126/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1266 - accuracy: 0.9540 - val_loss: 0.4878 - val_accuracy: 0.8746\n",
      "Epoch 127/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1303 - accuracy: 0.9506 - val_loss: 0.4791 - val_accuracy: 0.8644\n",
      "Epoch 128/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1189 - accuracy: 0.9544 - val_loss: 0.4573 - val_accuracy: 0.8712\n",
      "Epoch 129/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1111 - accuracy: 0.9619 - val_loss: 0.5280 - val_accuracy: 0.8610\n",
      "Epoch 130/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1136 - accuracy: 0.9574 - val_loss: 0.4869 - val_accuracy: 0.8610\n",
      "Epoch 131/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1151 - accuracy: 0.9627 - val_loss: 0.5144 - val_accuracy: 0.8610\n",
      "Epoch 132/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1174 - accuracy: 0.9612 - val_loss: 0.4840 - val_accuracy: 0.8542\n",
      "Epoch 133/250\n",
      "75/83 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9546INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.1247 - accuracy: 0.9537 - val_loss: 0.5114 - val_accuracy: 0.8847\n",
      "Epoch 134/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1020 - accuracy: 0.9680 - val_loss: 0.4896 - val_accuracy: 0.8576\n",
      "Epoch 135/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1269 - accuracy: 0.9514 - val_loss: 0.5011 - val_accuracy: 0.8542\n",
      "Epoch 136/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0907 - accuracy: 0.9706 - val_loss: 0.4812 - val_accuracy: 0.8746\n",
      "Epoch 137/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1130 - accuracy: 0.9616 - val_loss: 0.4707 - val_accuracy: 0.8644\n",
      "Epoch 138/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1072 - accuracy: 0.9672 - val_loss: 0.4620 - val_accuracy: 0.8678\n",
      "Epoch 139/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1032 - accuracy: 0.9627 - val_loss: 0.5192 - val_accuracy: 0.8610\n",
      "Epoch 140/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0970 - accuracy: 0.9695 - val_loss: 0.4811 - val_accuracy: 0.8746\n",
      "Epoch 141/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1151 - accuracy: 0.9593 - val_loss: 0.4489 - val_accuracy: 0.8780\n",
      "Epoch 142/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0892 - accuracy: 0.9706 - val_loss: 0.5296 - val_accuracy: 0.8780\n",
      "Epoch 143/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0992 - accuracy: 0.9650 - val_loss: 0.4718 - val_accuracy: 0.8712\n",
      "Epoch 144/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0888 - accuracy: 0.9714 - val_loss: 0.5958 - val_accuracy: 0.8610\n",
      "Epoch 145/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1115 - accuracy: 0.9597 - val_loss: 0.4759 - val_accuracy: 0.8712\n",
      "Epoch 146/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.1058 - accuracy: 0.9665 - val_loss: 0.5631 - val_accuracy: 0.8271\n",
      "Epoch 147/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.1371 - accuracy: 0.9510 - val_loss: 0.5980 - val_accuracy: 0.8407\n",
      "Epoch 148/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0982 - accuracy: 0.9657 - val_loss: 0.5106 - val_accuracy: 0.8746\n",
      "Epoch 149/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0992 - accuracy: 0.9680 - val_loss: 0.4957 - val_accuracy: 0.8847\n",
      "Epoch 150/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0910 - accuracy: 0.9676 - val_loss: 0.5391 - val_accuracy: 0.8678\n",
      "Epoch 151/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0993 - accuracy: 0.9638 - val_loss: 0.4960 - val_accuracy: 0.8712\n",
      "Epoch 152/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0860 - accuracy: 0.9725 - val_loss: 0.5471 - val_accuracy: 0.8780\n",
      "Epoch 153/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0757 - accuracy: 0.9778 - val_loss: 0.4701 - val_accuracy: 0.8814\n",
      "Epoch 154/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0961 - accuracy: 0.9661 - val_loss: 0.5217 - val_accuracy: 0.8712\n",
      "Epoch 155/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0847 - accuracy: 0.9710 - val_loss: 0.5519 - val_accuracy: 0.8576\n",
      "Epoch 156/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0975 - accuracy: 0.9672 - val_loss: 0.5248 - val_accuracy: 0.8644\n",
      "Epoch 157/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0820 - accuracy: 0.9770 - val_loss: 0.5752 - val_accuracy: 0.8814\n",
      "Epoch 158/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0768 - accuracy: 0.9785 - val_loss: 0.4763 - val_accuracy: 0.8814\n",
      "Epoch 159/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.5563 - val_accuracy: 0.8644\n",
      "Epoch 160/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0782 - accuracy: 0.9740 - val_loss: 0.5460 - val_accuracy: 0.8644\n",
      "Epoch 161/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0779 - accuracy: 0.9763 - val_loss: 0.5058 - val_accuracy: 0.8746\n",
      "Epoch 162/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0683 - accuracy: 0.9785 - val_loss: 0.5577 - val_accuracy: 0.8712\n",
      "Epoch 163/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1424 - accuracy: 0.9518 - val_loss: 0.6707 - val_accuracy: 0.8508\n",
      "Epoch 164/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.1418 - accuracy: 0.9472 - val_loss: 0.5835 - val_accuracy: 0.8475\n",
      "Epoch 165/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1264 - accuracy: 0.9570 - val_loss: 0.6048 - val_accuracy: 0.8644\n",
      "Epoch 166/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0783 - accuracy: 0.9766 - val_loss: 0.5540 - val_accuracy: 0.8678\n",
      "Epoch 167/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0859 - accuracy: 0.9687 - val_loss: 0.5422 - val_accuracy: 0.8610\n",
      "Epoch 168/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0774 - accuracy: 0.9736 - val_loss: 0.5879 - val_accuracy: 0.8644\n",
      "Epoch 169/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0652 - accuracy: 0.9804 - val_loss: 0.5546 - val_accuracy: 0.8746\n",
      "Epoch 170/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0793 - accuracy: 0.9736 - val_loss: 0.5885 - val_accuracy: 0.8746\n",
      "Epoch 171/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0785 - accuracy: 0.9744 - val_loss: 0.5902 - val_accuracy: 0.8644\n",
      "Epoch 172/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0754 - accuracy: 0.9729 - val_loss: 0.6161 - val_accuracy: 0.8610\n",
      "Epoch 173/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0692 - accuracy: 0.9781 - val_loss: 0.5605 - val_accuracy: 0.8780\n",
      "Epoch 174/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0547 - accuracy: 0.9879 - val_loss: 0.5164 - val_accuracy: 0.8746\n",
      "Epoch 175/250\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.6536 - val_accuracy: 0.8610\n",
      "Epoch 176/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0603 - accuracy: 0.9819 - val_loss: 0.5805 - val_accuracy: 0.8678\n",
      "Epoch 177/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0605 - accuracy: 0.9849 - val_loss: 0.5650 - val_accuracy: 0.8780\n",
      "Epoch 178/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0651 - accuracy: 0.9808 - val_loss: 0.5621 - val_accuracy: 0.8542\n",
      "Epoch 179/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0588 - accuracy: 0.9846 - val_loss: 0.6300 - val_accuracy: 0.8678\n",
      "Epoch 180/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0564 - accuracy: 0.9830 - val_loss: 0.5858 - val_accuracy: 0.8610\n",
      "Epoch 181/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0595 - accuracy: 0.9830 - val_loss: 0.5946 - val_accuracy: 0.8644\n",
      "Epoch 182/250\n",
      "79/83 [===========================>..] - ETA: 0s - loss: 0.0677 - accuracy: 0.9771INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.0686 - accuracy: 0.9766 - val_loss: 0.5688 - val_accuracy: 0.8881\n",
      "Epoch 183/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0610 - accuracy: 0.9812 - val_loss: 0.5786 - val_accuracy: 0.8678\n",
      "Epoch 184/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0612 - accuracy: 0.9804 - val_loss: 0.5146 - val_accuracy: 0.8746\n",
      "Epoch 185/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0786 - accuracy: 0.9706 - val_loss: 0.6096 - val_accuracy: 0.8576\n",
      "Epoch 186/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0794 - accuracy: 0.9706 - val_loss: 0.5387 - val_accuracy: 0.8746\n",
      "Epoch 187/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 0.5936 - val_accuracy: 0.8678\n",
      "Epoch 188/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0638 - accuracy: 0.9781 - val_loss: 0.6322 - val_accuracy: 0.8678\n",
      "Epoch 189/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0716 - accuracy: 0.9732 - val_loss: 0.5553 - val_accuracy: 0.8610\n",
      "Epoch 190/250\n",
      "77/83 [==========================>...] - ETA: 0s - loss: 0.0792 - accuracy: 0.9704INFO:tensorflow:Assets written to: ./weights/assets\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.0810 - accuracy: 0.9702 - val_loss: 0.5693 - val_accuracy: 0.8915\n",
      "Epoch 191/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0660 - accuracy: 0.9774 - val_loss: 0.6008 - val_accuracy: 0.8847\n",
      "Epoch 192/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.6298 - val_accuracy: 0.8644\n",
      "Epoch 193/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0647 - accuracy: 0.9797 - val_loss: 0.6082 - val_accuracy: 0.8881\n",
      "Epoch 194/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.6578 - val_accuracy: 0.8678\n",
      "Epoch 195/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0452 - accuracy: 0.9891 - val_loss: 0.6222 - val_accuracy: 0.8678\n",
      "Epoch 196/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0473 - accuracy: 0.9846 - val_loss: 0.6389 - val_accuracy: 0.8576\n",
      "Epoch 197/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0939 - accuracy: 0.9661 - val_loss: 0.6489 - val_accuracy: 0.8610\n",
      "Epoch 198/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0880 - accuracy: 0.9687 - val_loss: 0.7061 - val_accuracy: 0.8576\n",
      "Epoch 199/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0873 - accuracy: 0.9676 - val_loss: 0.7399 - val_accuracy: 0.8475\n",
      "Epoch 200/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0616 - accuracy: 0.9797 - val_loss: 0.6402 - val_accuracy: 0.8678\n",
      "Epoch 201/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0617 - accuracy: 0.9819 - val_loss: 0.6712 - val_accuracy: 0.8576\n",
      "Epoch 202/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0625 - accuracy: 0.9815 - val_loss: 0.6595 - val_accuracy: 0.8644\n",
      "Epoch 203/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0499 - accuracy: 0.9864 - val_loss: 0.5609 - val_accuracy: 0.8712\n",
      "Epoch 204/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0431 - accuracy: 0.9887 - val_loss: 0.6011 - val_accuracy: 0.8847\n",
      "Epoch 205/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0438 - accuracy: 0.9868 - val_loss: 0.5715 - val_accuracy: 0.8847\n",
      "Epoch 206/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0818 - accuracy: 0.9683 - val_loss: 0.7783 - val_accuracy: 0.8542\n",
      "Epoch 207/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1306 - accuracy: 0.9510 - val_loss: 0.7357 - val_accuracy: 0.8542\n",
      "Epoch 208/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0844 - accuracy: 0.9702 - val_loss: 0.6504 - val_accuracy: 0.8712\n",
      "Epoch 209/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0903 - accuracy: 0.9668 - val_loss: 0.6356 - val_accuracy: 0.8712\n",
      "Epoch 210/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.6342 - val_accuracy: 0.8644\n",
      "Epoch 211/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0461 - accuracy: 0.9868 - val_loss: 0.5775 - val_accuracy: 0.8746\n",
      "Epoch 212/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0354 - accuracy: 0.9921 - val_loss: 0.6060 - val_accuracy: 0.8847\n",
      "Epoch 213/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0341 - accuracy: 0.9910 - val_loss: 0.6868 - val_accuracy: 0.8678\n",
      "Epoch 214/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0549 - accuracy: 0.9804 - val_loss: 0.6887 - val_accuracy: 0.8678\n",
      "Epoch 215/250\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 0.0453 - accuracy: 0.9883 - val_loss: 0.6489 - val_accuracy: 0.8542\n",
      "Epoch 216/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0444 - accuracy: 0.9861 - val_loss: 0.6155 - val_accuracy: 0.8712\n",
      "Epoch 217/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0449 - accuracy: 0.9853 - val_loss: 0.6676 - val_accuracy: 0.8678\n",
      "Epoch 218/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0642 - accuracy: 0.9789 - val_loss: 0.5969 - val_accuracy: 0.8847\n",
      "Epoch 219/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.0386 - accuracy: 0.9902 - val_loss: 0.6983 - val_accuracy: 0.8678\n",
      "Epoch 220/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0616 - accuracy: 0.9827 - val_loss: 0.7073 - val_accuracy: 0.8542\n",
      "Epoch 221/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.6470 - val_accuracy: 0.8780\n",
      "Epoch 222/250\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.0305 - accuracy: 0.9928 - val_loss: 0.6155 - val_accuracy: 0.8915\n",
      "Epoch 223/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 0.7898 - val_accuracy: 0.8542\n",
      "Epoch 224/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0451 - accuracy: 0.9838 - val_loss: 0.6585 - val_accuracy: 0.8746\n",
      "Epoch 225/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.6092 - val_accuracy: 0.8746\n",
      "Epoch 226/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0344 - accuracy: 0.9913 - val_loss: 0.7573 - val_accuracy: 0.8610\n",
      "Epoch 227/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0369 - accuracy: 0.9898 - val_loss: 0.7167 - val_accuracy: 0.8678\n",
      "Epoch 228/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0766 - accuracy: 0.9751 - val_loss: 0.8036 - val_accuracy: 0.8508\n",
      "Epoch 229/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0542 - accuracy: 0.9812 - val_loss: 0.7092 - val_accuracy: 0.8780\n",
      "Epoch 230/250\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.0510 - accuracy: 0.9827 - val_loss: 0.6953 - val_accuracy: 0.8712\n",
      "Epoch 231/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.0395 - accuracy: 0.9894 - val_loss: 0.6257 - val_accuracy: 0.8780\n",
      "Epoch 232/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0487 - accuracy: 0.9849 - val_loss: 0.6557 - val_accuracy: 0.8576\n",
      "Epoch 233/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0403 - accuracy: 0.9887 - val_loss: 0.6735 - val_accuracy: 0.8746\n",
      "Epoch 234/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0329 - accuracy: 0.9925 - val_loss: 0.7126 - val_accuracy: 0.8678\n",
      "Epoch 235/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0741 - accuracy: 0.9744 - val_loss: 0.7009 - val_accuracy: 0.8678\n",
      "Epoch 236/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0309 - accuracy: 0.9913 - val_loss: 0.6864 - val_accuracy: 0.8644\n",
      "Epoch 237/250\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.0275 - accuracy: 0.9936 - val_loss: 0.7811 - val_accuracy: 0.8712\n",
      "Epoch 238/250\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 0.0708 - accuracy: 0.9763 - val_loss: 0.8840 - val_accuracy: 0.8271\n",
      "Epoch 239/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0849 - accuracy: 0.9714 - val_loss: 0.7151 - val_accuracy: 0.8610\n",
      "Epoch 240/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0430 - accuracy: 0.9808 - val_loss: 0.7731 - val_accuracy: 0.8780\n",
      "Epoch 241/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0364 - accuracy: 0.9910 - val_loss: 0.6734 - val_accuracy: 0.8814\n",
      "Epoch 242/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0213 - accuracy: 0.9970 - val_loss: 0.6339 - val_accuracy: 0.8881\n",
      "Epoch 243/250\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0219 - accuracy: 0.9955 - val_loss: 0.6614 - val_accuracy: 0.8746\n",
      "Epoch 244/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0228 - accuracy: 0.9951 - val_loss: 0.6574 - val_accuracy: 0.8780\n",
      "Epoch 245/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.7951 - val_accuracy: 0.8610\n",
      "Epoch 246/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0280 - accuracy: 0.9940 - val_loss: 0.6943 - val_accuracy: 0.8678\n",
      "Epoch 247/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0252 - accuracy: 0.9951 - val_loss: 0.7322 - val_accuracy: 0.8746\n",
      "Epoch 248/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0268 - accuracy: 0.9928 - val_loss: 0.6465 - val_accuracy: 0.8847\n",
      "Epoch 249/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0166 - accuracy: 0.9981 - val_loss: 0.6946 - val_accuracy: 0.8712\n",
      "Epoch 250/250\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0172 - accuracy: 0.9977 - val_loss: 0.7136 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14beec400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=250,validation_split=0.1, callbacks=[\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='./weights', monitor='val_accuracy', save_best_only=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pINJqO3uOdpJ",
    "outputId": "aadecd35-073e-4e14-f995-2c5a5647b076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15bae4760>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 6ms/step - loss: 0.1033 - accuracy: 0.9742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10327724367380142, 0.97422856092453]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7466 - accuracy: 0.8482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7465741634368896, 0.8482385277748108]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
